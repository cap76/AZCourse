[
["index.html", "Classical approaches to Machine Learning 1 About the course 1.1 Prerequisites 1.2 Schedule 1.3 Github 1.4 Google docs interactive Q&amp;A 1.5 License 1.6 Contact 1.7 Colophon", " Classical approaches to Machine Learning Chris Penfold 2021-11-28 1 About the course Machine learning describes a series of data-driven algorithmic approaches that simulate the “learning without being explicitly programmed” paradigm. These methods are particularly useful when limited information is available about the structure or properties of a dataset; also, real-world data rarely follows a well-defined mathematical distribution (due to technical variation in measurements, noise, etc) – assumption-free models offer flexibility for this type of input with the side effects of underlying characteristics of the dataset (e.g. through feature selection). The term “Machine Learning” encompasses a broad range of approaches in data analysis with wide applicability across biological sciences. Lectures will introduce commonly used approaches, provide insight into their theoretical underpinnings and illustrate their applicability and limitations through examples and exercises. During the practical sessions students will apply the algorithms to real biological data-sets using the R language and RStudio environment. All code utilised during the course will be available to participants. 1.1 Prerequisites Some familiarity with R would be helpful. 1.2 Schedule Time Data Module 14:00 – 15:00 30/11/21 Linear regression / linear models 15:00 – 16:30 30/11/21 Logistic regression 16:30 – 17:00 30/11/21 Review and questions 14:00 – 15:00 2/12/21 Artificial Neural Networks 15:00 – 16:30 2/12/21 Convolutional neural nets and beyond 16:30 – 17:00 2/12/21 Review and questions 1.3 Github The github reposotory for Classical approaches to Machine Learning containing code, datasets and lectures is availabile here. The html textbook is found in docs/index.html. Individual chapters (files ending .Rmd) can be opened in RStudio as interactive markdown files. 1.4 Google docs interactive Q&amp;A Clicking the link [here]{https://docs.google.com/document/d/1fDiVihZWsSiFKllsANFGd_jV3SK6jlq4IOj5dEPtQTM/edit?usp=sharing} takes you to the interactive Q&amp;A document, where you can ask any questions you might have. 1.5 License GPL-3 1.6 Contact If you have any comments, questions or suggestions about the material, please contact Chris Penfold. 1.7 Colophon This book was produced using the bookdown package (Xie 2017), which was built on top of R Markdown and knitr (Xie 2015). References "],
["intro.html", "2 Introduction", " 2 Introduction In this workbook, we cover two themes. We begin in chapter 3 with linear regression and logistic regression. Here we will explore how to use these statistical methods in the context of machine learning. The excercises are based on a plant infection dataset. We will see how regression models of different complexity can be fit to the dataset, and how held out data can be used to choose the best model. After this, we will see how logistic regression approaches can be used to pick out marker genes that indicate infected status. In chapter 4 we introduce the concepts of neural networks. We demonstrate how neural networks can be built using the {kerasR} package for regression applications. Later we introduce Convolutional Neural Networks (CNN) and see how they can be used for image recognition applications. In this chapter we attempt to build an algorithm capable of recognising a well known cartoon characer in a se of images. Finally, we briefly discuss how these basic approaches can be built into more complex algorithms. "],
["logistic-regression.html", "3 Linear regression and logistic regression 3.1 Regression 3.2 Resources", " 3 Linear regression and logistic regression Supervised learning refers to the general task of identifying how a set of annotated input data maps to a set of outcomes. In other words, it’s about learning functions from a labelled set of data, and using those functions for prediction. The labelled data typically consists of a matched pair of observations \\(\\{\\mathbf{X},\\mathbf{y}\\}\\), where \\(\\mathbf{X}\\) (the input variables) is usually a matrix of (real-valued) explanatory variables, with \\(\\mathbf{X}_i\\) denoting the \\(i\\)th column which contains observations for the \\(i\\)th variable, and \\(\\mathbf{y} = (y_1,\\ldots,y_n)^\\top\\) (the output variable) denotes a vector of observations for a variable of interest (the input variables need not be real values vectors, and could instead represent any measurement including graphs, text etc.). In general, the input variables are often easier or cheaper to measure, but it’s the output variables that we’re really interested in. Depending on the nature of the output variable, supervised learning is generally split into regression and classification tasks. Within a regression setting, we usually aim to identify how the input variables map to the (continuous-valued) output variable(s). A simple example would involve measuring the population size of a bacterial culture, \\(\\mathbf{y} = (N_1,\\ldots,N_n)^\\top\\), at a set of time points, \\(\\mathbf{X} = (t_1,\\ldots,t_n)^\\top\\), and learning the function that maps from \\(\\mathbf{X}\\) to \\(\\mathbf{y}\\). Doing so should reveal something about the physical nature of the system, such as identifying the existence of distinct phases of growth. Correctly identifying these functions would also allow us to predict the output variable, \\(\\mathbf{y}^* = (N_i^*,\\ldots,N_k^*)^\\top\\), at a new set of times, \\(\\mathbf{X}^* = (t_i,\\ldots,t_k)^\\top\\). Classification algorithms, on the other hand, deal with discrete-valued outputs. Here each observation in \\(\\mathbf{y} = (y_1,\\ldots,y_n)\\) can take on only a finite number of values. For example, we may have a measurement that indicates “infected” versus “uninfected”, which can be represented in binary, \\(y_i \\in [0,1]\\). More generally we have data that falls into \\(K\\) classes e.g., “group 1” through to “group K”. As with regression, the aim is to identify how the (potentially continuous-valued) input variables map to the discrete set of class labels, and ultimately, assign labels to a new set of observations. Notable examples would be to identify how the expression levels of particular set of marker genes are predictive of a discrete phenotype. Although we do not specifically addrerss classification in this section, towards the end we will cover a regression model that can be used to perform binary classification from continuous values input data: logistic regression. Logistic regression, itself, is not a classification algorithm per se, but it can be used in such a context, as we will show. In section 3.1 we briefly recap linear regression. As an example, we demonstrate the use of regression to predict gene expression values as a function of time, and how this can be used to inform us about the nature of the data, and as a way to make decisions about whether there are changes in gene expression over time.Then, in section 3.1.4 we introduce logistic regression (section 3), and demonstrate how such approaches can be used to predict pathogen infection status in Arabidopsis thaliana, again based on gene expression levels. By doing so we identify key marker genes indicative of pathogen growth. 3.1 Regression In this section, we will recap our understanding of regression. To do so will make use of an existing dataset which captures the gene expression levels in the model plant Arabidopsis thaliana following innoculation with Botrytis cinerea (Windram et al. 2012), a necrotrophic pathogen considered to be one of the most important fungal plant pathogens due to its ability to cause disease in a range of plants. Specifically this dataset is a time series measuring the gene expression of 100 or so genes in Arabidopsis leaves following inoculation with Botrytis cinerea over a \\(48\\) hour time window, with observations taken at \\(2\\) hour intervals. Whilst this example is distinctly biological in motivation the methods we discuss should be general and applicable to other collections of time series data, and it may be helpful to instead think of things in terms of input variables and output variables. The dataset is available from GEO (GSE39597) but a pre-processed version has been deposited in the {data} folder. This pre-processed data contains the expression levels of a set of \\(163\\) marker genes in tab delimited format. The fist row contains gene IDs for the marker genes (the individual input variables). Column \\(2\\) contains the time points of observations, with column \\(3\\) containing a binary indication of infection status evalutated as \\(0\\) or \\(1\\) according to wether there was a detectable presence of Botrytis cinerea tubulin protein. All subsequent columns indicate (\\(\\log_2\\)) normalised Arabidopsis gene expression values from microarrays (V4 TAIR V9 spotted cDNA array). The expression dataset itself contains two time series: the first \\(24\\) observations represent measurements of Arabidopsis gene expression in a control time series (uninfected), from \\(2h\\) through \\(48h\\) at \\(2\\)-hourly intervals, and therefore capture dynamic aspects natural plant processes, including circadian rhythms; the second set of \\(24\\) observations represents an infected dataset, again commencing \\(2h\\) after inoculation with Botyris cinerea through to \\(48h\\). Within this section our output variable will typically be the expression level of a particular gene of interest, denoted \\(\\mathbf{y} =(y_1,\\ldots,y_n)^\\top\\), with the explanatory variable being time, \\(\\mathbf{X} =(t_1,\\ldots,t_n)^\\top\\). We can read the dataset into {R} as follows: D &lt;- read.csv(file = &quot;data/Arabidopsis/Arabidopsis_Botrytis_transpose_3.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) We can also extract out the names of the variables (gene names), and the unique vector of measurment times. For the control experiment this would look like: genenames &lt;- colnames(D) Xs &lt;- D$Time[1:24] whilst for the treatment, it would be genenames &lt;- colnames(D) Xs2 &lt;- D$Time[25:nrow(D)] Exercise 1.1. Plot the gene expression profiles to familiarise yourself with the data. No, really, plot the data. This is always the first thing you should be doing with your datasets - look at them. The general aim of this module is to give you hands on experience with linear and logistic regression and cover a number of other concepts from machine learning. The main points are: Do you understand the data? A priority before doing any ML should be to look, interrogate, and understand the data. What is the data. Look at the data. Repeat 1-3 a few more times. Once more for luck, look at the data. Once we begin to understand the data, we will have a better grasp of what we are doing and what ML approaches to take. In the first example we will start with some linear regression. We will explore more complicated forms of regression. We will get an idea of how to partition data into training and test sets, and how in doing so we may distinguish between different types of models. We will start to get an intuition for how regression tasks may be used to predict values an new locations, and why choosing the right kind of model is important. 3.1.1 Linear regression Now that we have an idea about what our dataset is, we can start to do something with it. Here we have a time series (a number of time-series, in fact), so we may, at some point, want to develop a models of how specific genes are changing over time: this would allow us to predict what gene expression might be doing at some point in the futurer (forecasting) or uncover something about the physical nature of the system i.e., what kind of function best describes the behaviour. Recall that one of the simplest forms of regression, linear regression, assumes that the variable of interest, \\(y\\), depends on an explanatory variable, \\(x\\), via: \\(y = m x + c.\\) For a typical set of data, we have a vector of observations, \\(\\mathbf{y} = (y_1,y_2,\\ldots,y_n)\\) with a corresponding set of explanatory variables. For now we can assume that the explanatory variable is scalar, for example time (in hours), such that we have a set of observations, \\(\\mathbf{X} = (t_1,t_2,\\ldots,t_n)\\). Using linear regression we aim to infer the parameters \\(m\\) and \\(c\\), which will tell us something about the relationship between the two variables, and allow us to make predictions at a new set of locations, \\(\\mathbf{X}*\\). Within {R}, linear regression can be implemented via the {lm} function. In the example below, we perform linear regression for the gene expression of AT2G28890 as a function of time, using the infection time series only (hence we use only the first \\(24\\) datapoints): linmod &lt;- lm(AT2G28890~Time, data = D[25:nrow(D),]) within this snippet of code, the {lm} function has analytically identified the gradient and offset (\\(m\\) and \\(c\\) parameters) based upon all 24 time points, and we can take a look at those parameters via {linmod$oefficients}. In general, it is not a very good idea to infer parameters using all of the data. Doing so would leave no way to evaluate for overfitting. Ideally, we wish to partition the dataset into a training set, and an evaluation set, with parameters evaluated on the training set, and model performance summarised over the evaluation set. We can of course partition this dataset manually, or use a package to do so. In the previous workshops we saw how {caret} machine learning wrapper could be used to easily specify various partitions of the dataset. Linear regression is implemented within the {caret} package, allowing us to make use of these utilities. In fact, within {caret}, linear regression is performed by calling the function {lm}. In the example, below, we perform linear regression for gene AT2G28890, and predict the expression pattern for that gene using the {predict} function: library(caret) ## Warning: package &#39;caret&#39; was built under R version 3.5.2 ## Loading required package: lattice ## Warning: package &#39;lattice&#39; was built under R version 3.5.2 ## Loading required package: ggplot2 library(mlbench) library(ggplot2) set.seed(1) geneindex &lt;- which(genenames==&quot;AT2G28890&quot;) lrfit &lt;- train(y~., data=data.frame(x=Xs,y=D[25:nrow(D),geneindex]), method = &quot;lm&quot;) predictedValues&lt;-predict(lrfit) A summary of the model, including parameters, can be printed out to screen using the {summary} function: summary(lrfit) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77349 -0.17045 -0.01839 0.15795 0.63098 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.14010 0.13975 72.56 &lt; 2e-16 *** ## x -0.04997 0.00489 -10.22 8.14e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3317 on 22 degrees of freedom ## Multiple R-squared: 0.826, Adjusted R-squared: 0.8181 ## F-statistic: 104.4 on 1 and 22 DF, p-value: 8.136e-10 Conveniently, in cases where we do not specify a split in the data, {caret} will split these by default settings, and can look at various metrics on the held out data in {lrfit$results}. We can make predictions at new points (for example if we are interested in forecasting at some time in the future) by specifying a new set of time points over which to make a prediciton: newX &lt;- c(49,50,51,52,53,54) forecastValues&lt;-predict(lrfit,newdata = data.frame(x=newX) ) Let’s also fit a linear model to the control dataset, and plot the inferred results alongside the observation data for both fitted models: lrfit2 &lt;- train(y~., data=data.frame(x=Xs,y=D[1:24,geneindex]), method = &quot;lm&quot;) predictedValues2 &lt;- predict(lrfit2) plot(Xs,D[25:nrow(D),geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) points(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;red&quot;) points(Xs,predictedValues,type=&quot;l&quot;,col=&quot;black&quot;) points(Xs,predictedValues2,type=&quot;l&quot;,col=&quot;red&quot;) Whilst the above model appeared to do reasonably well at capturing the general trends in the dataset, if we take a closer look at the control data (in red), you may notice that, visually, there appears to be more structure to the data than indicated by the model fit. Indeed, if we look AT2G28890 up on CircadianNET, we will see it is likely circadian in nature (\\(p&lt;5\\times10^{-5}\\)) suggesting there may be some rhythmicity to it. To better accommodate the complex nature of this data we may need something more complicated. 3.1.2 Polynomial regression In general, linear models will not be appropriate for a large variety of datasets, particularly when the variables of interest are nonlinear. We can instead try to fit more complex models, such as a quadratic function, which has the following form: \\(y = m_1 x + m_2 x^2 + c,\\) where \\(m = [m_1,m_2,c]\\) represent the parameters we’re interested in inferring. An \\(n\\)th-order polynomial has the form: \\(y = \\sum_{i=1}^{n} m_i x^i + c.\\) where \\(m = [m_1,\\ldots,m_n,c]\\) are the free parameters. Within {R} we can infer more complex polynomials to the data using the {lm} package by calling the {poly} function when specifying the symbolic model. In the example below we fit a \\(3\\)rd order polynomial (the order of the polynomial is specified via the {degree} variable): lrfit3 &lt;- lm(y~poly(x,degree=3), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) We can do this within {caret}: in the snippet, below, we fit \\(3\\)rd order polynomials to the control and infected datasets, and plot the fits alongside the data. lrfit3 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) lrfit4 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[25:nrow(D),1],y=D[25:nrow(D),geneindex]), method = &quot;lm&quot;) plot(Xs,D[25:nrow(D),geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2,max(D[,geneindex]+0.2)),main=genenames[geneindex]) points(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;red&quot;) lines(Xs,fitted(lrfit3),type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,fitted(lrfit4),type=&quot;l&quot;,col=&quot;black&quot;) Note that, by eye, the fit appears to be a little better than for the linear regression model. Well, maybe! We can quantify the accuracy of the models by looking at the root-mean-square error (RMSE) on the hold-out data (cross validation), defined as: \\(\\mbox{RMSE} = \\sqrt{\\sum_{i=1}^n (\\hat{y_i}-y_i)^2/n}\\) where \\(\\hat{y_i}\\) is the predicted value (model prediction) and \\(y_i\\) the observed value of the \\(i\\)th (held out) datapoint. What happens if we fit a much higher order polynomial? Try fitting a polynomial with degree \\(d = 12\\) and plotting the result. As we increase the model complexity the fit appears to match perfectly well in the training set, but becomes completely useless for prediction. We are overfitting! This is why we use held out data, so that we can evaluate, empirically, when a model is useful, or when it is simply memorising the training set. Exercise 1.2. Using your gene of interest explore the model complexity i.e., try fitting polynomial models of increasing complexity. Plot the RMSE on the test sets as a function of degree. Which model fits best? 3.1.3 Distributions of fits In the previous section we explored fitting a polynomial function to the data. Recall that we can fit a \\(4\\)th order polynomial to the control datasets as follows: lrfit3 &lt;- lm(y~poly(x,degree=4), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) plot(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) lines(Xs,fitted(lrfit3),type=&quot;l&quot;,col=&quot;red&quot;) It looks reasonable, but how does it compare to the following shown in blue? lrfit4 &lt;- lrfit3 lrfit4$coefficients &lt;- lrfit4$coefficients + 0.1*matrix(rnorm(length(lrfit4$coefficients)),length(lrfit4$coefficients)); pred1&lt;-predict(lrfit4, data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) plot(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) lines(Xs,fitted(lrfit3),type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,pred1,type=&quot;l&quot;,col=&quot;blue&quot;) Our new fit was generated by slightly perturbing the optimised parameters via the addition of a small amount of noise. We can see that the new fit is almost as good, and will have a very similar SSE[^This should give us some intuition on the notion of over-fitting. For example, if we make a small perturbation to the parameters of a simpler model, the function will not change all that much; on the other hand, if we made a small perturbation to the parameters of a more complex polynomial, the function may look drastically different. To explain the data with the more complex model would therefore require very specific sets of parameters]. In general, inferring a single fit to a model is prone to overfitting. A much better approach is to instead fit a distribution over fits. We can generate samples from a linear model using the {coef} function. To do so we must use the {lm} function directly, and not via the {caret} package. library(&quot;arm&quot;) ## Loading required package: MASS ## Warning: package &#39;MASS&#39; was built under R version 3.5.2 ## Loading required package: Matrix ## Warning: package &#39;Matrix&#39; was built under R version 3.5.2 ## Loading required package: lme4 ## Warning: package &#39;lme4&#39; was built under R version 3.5.2 ## ## arm (Version 1.12-2, built: 2021-10-15) ## Working directory is /Users/christopherpenfold/Desktop/AZMachineLearning/intro-machine-learning lrfit4 &lt;- lm(y~poly(x,degree=4), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) simulate &lt;- coef(sim(lrfit4)) paramsamp &lt;- head(simulate,10) This will sample model parameters that are likely to explain the dataset. In this case we have produced \\(10\\) different sets of sample parameters. In the code, below, we plot those \\(10\\) sample polynomials: plot(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) for (i in c(1,2,3,4,5,6,7,8,9,10)){ lrfit4$coefficients &lt;- paramsamp[i,] pred1&lt;-predict(lrfit4, data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) lines(Xs,pred1,type=&quot;l&quot;,col=&quot;red&quot;) } Alternatively, we can visualise the confidence bounds directly: lrfit4 &lt;- lm(y~poly(x,degree=4), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex])) pred1&lt;-predict(lrfit4, interval=&quot;predict&quot;) ## Warning in predict.lm(lrfit4, interval = &quot;predict&quot;): predictions on current data refer to _future_ responses plot(Xs,D[1:24,geneindex],type=&quot;p&quot;,col=&quot;black&quot;,ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex]) lines(Xs,pred1[,1],type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,pred1[,2],type=&quot;l&quot;,col=&quot;red&quot;) lines(Xs,pred1[,3],type=&quot;l&quot;,col=&quot;red&quot;) 3.1.4 Logistic regression The type of linear regression models we’ve been using up to this point deal with real-valued observation data, \\(\\mathbf{y}\\), and are therefore not appropriate for classification. To deal with cases where \\(\\mathbf{y}\\) is a binary outcome, we instead have to think of different models. Logistic regression is one example which can be used to model data in which there is a general transition from one state to another as a function of the input variable e.g., where gene expression levels might predict disease state, with lower levels indicating disease-free, and higher-levels indicating a diseased state. Logistic regression does not perform classification per se, but instead models the probability of a successful event (e.g., a \\(1\\)). As probability is a real-valued number (between \\(0\\) and \\(1\\)), technically this remains a form of regression. However, we can logistic regression to make classifications by setting thresholds on the probabilities i.e., if we decide everything with \\(p\\ge 0.5\\) is a success (\\(1\\)), and everything below is a \\(0\\). Another way to think about linear regression is that we are fitting a linear model to the logit (natural log) of the log-odds ratio: \\(\\ln \\biggl{(}\\frac{p(x)}{1-p(x)}\\biggr{)} = c + m_1 x_1.\\) Although this model is not immediately intuitive, if we solve for \\(p(x)\\) we get: \\(p(x) = \\frac{1}{1+\\exp(-c - m_1 x_1)}\\). We have thus specified a function that indicates the probability of success for a given value of \\(x\\) e.g., \\(P(y=1|x)\\). In general can think of our data as a being a sample from a Bernoulli trial, and can therefore write down the likelihood for a set of observations \\({\\mathbf{X},\\mathbf{y}}\\): \\(\\mathcal{L}(c,m_1) = \\prod_{i=1}^n p(x_i)^{y_i} (1-p(x_i)^{1-y_i})\\). Unlike linear regression, these models do not admit a closed form solution, but can be solved iteratively via maximum likelihood. That is by finding the values \\((c,m_1)\\) that return the greatest value of \\(\\mathcal{L}(c,m_1)\\). Within {caret}, logistic regression can applied using the {glm} function. To illustate this we will again make use of our plant dataset. Recall that the second column represents a binary variable indicative of infection status e.g., population growth of the Botrytis cinerea pathogen indicated by observable Botrytis tubulin. In the excercises, below, we will use logistic regression to learn a set of markers capable of predicting infection status. To begin with, let’s see if time is informative of infection status: library(pROC) ## Warning: package &#39;pROC&#39; was built under R version 3.5.2 ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var library(ROCR) ## Loading required package: gplots ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess options(warn=-1) mod_fit &lt;- train(y ~ ., data=data.frame(x = D$Time, y = as.factor(D$Class)), method=&quot;glm&quot;, family=&quot;binomial&quot;) To make things easier, for model evaluation, we will load in a second (related) dataset, containing a new set of observations not seen by the model, and predict infection status on this held out data. Dpred &lt;- read.csv(file = &quot;data/Arabidopsis/Arabidopsis_Botrytis_pred_transpose_3.csv&quot;, header = TRUE, sep = &quot;,&quot;, row.names=1) prob &lt;- predict(mod_fit, newdata=data.frame(x = Dpred$Time, y = as.factor(Dpred$Class)), type=&quot;prob&quot;) pred &lt;- prediction(prob$`1`, as.factor(Dpred$Class)) To evaluate how well the algorithm has done, we can calculate a variety of summary statistics. For example the number of true positives, true negatives, false positives, and false negatives. A useful summary is to plot the ROC curve (false positive rate versus true positive rate) and calculate the area under the curve. Recall that for a perfect algorithm the area under this curve (AUC) will be equal to \\(1\\), whereas random assignment would give an area of \\(0.5\\). In the example below, we will calculate the AUC for a logistic regression model: perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) plot(perf) auc &lt;- performance(pred, measure = &quot;auc&quot;) auc &lt;- auc@y.values[[1]] auc ## [1] 0.6111111 Okay, so a score of \\(0.61\\) is certainly better than random, but not particularly good. This is perhaps not surprising, as half the time series (the control) is uninfected over the entirety of the time series, whilst in the second times series Botrytis is able to infect from around time point 8 onward. The slightly better than random performance therefore arises due the slight bias in the number of instances of each class. Indeed, if we plot infection status vs time, we should be able to see why the model fails to be predictive. In the example below, we instead try to regress infection status against individual gene expression levels. The idea is to identify genes that have expression values indicative of Botrytis infection: marker genes. aucscore &lt;- matrix(rep(0, 164), 1, 164) for (i in seq(3,164)){ mod_fit &lt;- train(y ~ ., data=data.frame(x = D[,i], y = as.factor(D$Class)), method=&quot;glm&quot;, family=&quot;binomial&quot;) prob &lt;- predict(mod_fit, newdata=data.frame(x = Dpred[,i], y = as.factor(Dpred$Class)), type=&quot;prob&quot;) pred &lt;- prediction(prob$`1`, as.factor(Dpred$Class)) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure = &quot;auc&quot;) aucscore[i] &lt;- auc@y.values[[1]] } plot(aucscore[1,3:ncol(aucscore)],ylab=&quot;AUC&quot;,xlab=&quot;gene index&quot;) We note that, several genes in the list appear to have AUC scores much greater than \\(0.6\\). We can take a look at some of the genes with high predictive power: genenames[which(aucscore&gt;0.8)] ## [1] &quot;AT1G29990&quot; &quot;AT1G67170&quot; &quot;AT2G21380&quot; &quot;AT2G28890&quot; &quot;AT2G35500&quot; &quot;AT2G45660&quot; ## [7] &quot;AT3G09980&quot; &quot;AT3G11590&quot; &quot;AT3G13720&quot; &quot;AT3G25710&quot; &quot;AT3G44720&quot; &quot;AT3G48150&quot; ## [13] &quot;AT4G00710&quot; &quot;AT4G02150&quot; &quot;AT4G16380&quot; &quot;AT4G19700&quot; &quot;AT4G26450&quot; &quot;AT4G28640&quot; ## [19] &quot;AT4G34710&quot; &quot;AT4G36970&quot; &quot;AT4G39050&quot; &quot;AT5G11980&quot; &quot;AT5G22630&quot; &quot;AT5G24660&quot; ## [25] &quot;AT5G43700&quot; &quot;AT5G50010&quot; &quot;AT5G56250&quot; Unsurprisingly, among these genes we see a variety whose proteins are known to be targeted by various pathogen effectors, and are therefore directly implicated in the immune response (Table 1). Gene Effector AT3G25710 | ATR1 ATR1_ASWA1 AT4G19700 | ATR1 ATR13_NOKS1 AT4G34710 | ATR1 ATR13_NOKS1 AT4G39050 | ATR1 ATR13_NOKS1 AT5G24660 | ATR1 ATR13_NOKS1 AT4G00710 | AvrR AvrRpt2_Pto JL1065_CatalyticDead AT4G16380 | HARX HARXL44 AT2G45660 | HARX HARXL45 AT5G11980 | HARX HARXL73 AT2G35500 | HARX HARXLL445 AT1G67170 | HARX HARXLL470_WACO9 AT4G36970 | HARX HARXLL470_WACO9 AT5G56250 | HARX HARXLL470_WACO9 AT3G09980 | HARX HARXLL516_WACO9 AT5G50010 | HARX HARXLL60 AT3G44720 | HARX HARXLL73_2_WACO9 AT5G22630 | HARX HARXLL73_2_WACO9 AT5G43700 | HopH HopH1_Psy B728A Table 1: Genes predictive of infection status of Botrytis cinerea whose proteins are targeted by effectors of a variety of pathogens Let’s take a look at what the data looks like. In this case we plot the training data labels and the fit from the logistic regression i.e., \\(p(\\mathbf{y}=1|\\mathbf{x})\\): bestpredictor &lt;- which(aucscore==max(aucscore)) best_mod_fit &lt;- train(y ~., data=data.frame(x = D[,bestpredictor], y = as.factor(D$Class)), family=&quot;binomial&quot;, method=&quot;glm&quot;) plot(D[,bestpredictor],D$Class,xlab=genenames[bestpredictor],ylab=&quot;Class&quot;) lines(seq(min(D[,bestpredictor]),max(D[,bestpredictor]),length=200),predict(best_mod_fit,newdata=data.frame(x = seq(min(D[,bestpredictor]),max(D[,bestpredictor]),length=200)),type=&quot;prob&quot;)[,2]) We can see from this plot that the level of AT3G44720 appears to be highly predictive of infection status. When AT3G44720 is highly expressed, its almost certain that the Botrytis cinerea has gained a foothold; whether this is causal or not, we cannot say, but it is almost certainly a good marker. Linear regression and logistic regression represent useful tools for dissecting relationships among variables, and are frequently used as tools to interpret complex datasets. There are some cases where linear approaches may not work so well, however. To illustrate this we will construct an artificial dataset in which low expression levels of a gene indicates no infection, with moderate levels indicating infection; very high levels of the gene, however, do not indicate infected status, but might only arise artificially, due to e.g., inducible overexpression. For this dataset very high levels are thus labeled as uninfected. Below we construct this in silico dataset based loosely on the expression levels of AT3G44720. xtrain = D[,bestpredictor] ytrain = as.numeric(D$Class) ytrain[which(xtrain&gt;12.5)]=0 ytrain[which(xtrain&lt;10)]=0 ytrain = as.factor(ytrain) xpred = Dpred[,bestpredictor] ypred = as.numeric(Dpred$Class) ypred[which(xpred&gt;12.5)]=0 ypred[which(xpred&lt;10)]=0 ypred = as.factor(ypred) Let’s fit a logistic model and visualise the result: mod_fit3 &lt;- train(y ~., data=data.frame(x = xtrain, y= as.factor(ytrain)), family=&quot;binomial&quot;, method = &quot;glm&quot;) plot(xtrain,as.numeric(ytrain)-1,xlab=&quot;Marker gene&quot;,ylab=&quot;Class&quot;) lines(seq(min(xtrain),max(xtrain),length=200),predict(mod_fit3,newdata=data.frame(x = seq(min(xtrain),max(xtrain),length=200),y= matrix(200,1,1)),type=&quot;prob&quot;)[,2]) mod_fit3$results$Accuracy ## [1] 0.8360583 We can see from the plot that the model fit is very poor. However, if we look at the accuracy (printed at the bottom) the result appears to be good. This is due to the skewed number of samples from each class: there are far more uninfected samples than there are infected, which means that if the model predicts uninfected for every instance, it will be correct more than it’s incorrect. We can similarly check the result on our test dataset: prob&lt;-predict(mod_fit3, newdata=data.frame(x =xpred, y = as.factor(ypred)), type=&quot;prob&quot;) pred &lt;- prediction(prob[,2], ypred) perf &lt;- performance(pred, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) auc &lt;- performance(pred, measure = &quot;auc&quot;) auc &lt;- auc@y.values[[1]] auc ## [1] 0.7104377 These results indicates two key issues. Class imbalance can be a particular problem in regression approaches, and it is important to monitor this on a practical level. Things that we can do, depending on the dataset and method, include up-weighting the less frequently sampled class, or discarding more of the dominant class. Other metrics may also be more appropriate that AUC when classes are highly unbalanced, including area under precision-recall curves. Secondly, nonlinearities in a dataset can be problematic for both linear regression and logistic regression, and a variety of approaches exist for tackling such problems under the umbrella term of nonlinear regression. 3.2 Resources A variety of examples using {caret} to perform regression and classification have been implemented here. For those that want to start their own reading on nonlinear regression, a good stating point is Rasmussen and William’s book on Gaussian processes. Be warned, it will contain a lot more maths than this course. ======= ## Exercises Solutions to exercises can be found in appendix 5. References "],
["mlnn.html", "4 Deep Learning 4.1 Multilayer Neural Networks 4.2 Convolutional neural networks 4.3 Further reading", " 4 Deep Learning 4.1 Multilayer Neural Networks Neural networks with multiple layers are increasingly used to attack a variety of complex problems in biology under the umbrella of deep learning (Angermueller and Stegle 2016, @Mohammad2019deep). This umbrella contains an incredibly diverse range of techniques, including densely connected networks (which are essentially complex counterparts to the traditional perceptron), convolutional neural networks (CNN), autoencoders (AE), and adversarial neural networks (ANN), amongst others. In this section we will explore the basics of deep learning on a practical level. We will first learn how to construct a neural network using {KerasR}. We will first use densely connected neural networks to explore a regression setting, before trying our hand at image classification using a set of images taken from the animated TV series Rick and Morty. For those unfamiliar with Rick and Morty, the series revolves around the adventures of Rick Sanchez, an alcoholic, arguably sociopathic scientist, and his neurotic grandson, Morty Smith. Although many scientists aspire to be like Rick, they’re usually more like a Jerry. Our motivating goal in this latte section is to develop an image classification algorithm capable of telling us whether any given image contains Rick or not: a binary classification task with two classes, Rick or not Rick. For training purposes I have downloaded several thousand random images of Rick and several thousand images without Rick from the website Master of All Science. The main ideas to take home from this section are: Look at the data. There are a limitless variety of architectures that can be built into a neural network. Picking one to use is often arbitrary or at best empirically-motivated by previous works. Some approaches are better suited for specific datasets. 4.1.1 Constructing Neural Networks with KerasR Before we get to our main task, we will first have a go at building simple densely connected Neural Networks (NN) to perform regression. In general the nature of the NN we use will be motivated by the dataset we have and the question we’re interested in. As a gentle intoduction, we aim to use NNs to calculate the square root of a number. We first generate training and evaluation dataset. For the training set we generate two arrays, an input array, containing a random set of numbers (between \\(0\\) and \\(100\\)), and an output array, containing the square roots of those numbers (a similar set will be independently generated for the test set): library(keras) library(jpeg) ## Warning: package &#39;jpeg&#39; was built under R version 3.5.2 library(grid) library(kerasR) ## successfully loaded keras ## ## Attaching package: &#39;kerasR&#39; ## The following objects are masked from &#39;package:keras&#39;: ## ## normalize, pad_sequences, text_to_word_sequence, to_categorical tdims &lt;- 50 #Number of samples to generate x &lt;- runif(tdims, min=0, max=100) #Generate random x in range 0 to 100 y &lt;- sqrt(x) #Calculate square root of x trainingX &lt;- array(0, dim=c(tdims,1)) #Store data as an array (required by Keras) trainingX[1:tdims,1] &lt;- x trainingY &lt;- array(0, dim=c(tdims,1)) trainingY[1:tdims,1] &lt;- y #Now do the same but for a independently generated test set x &lt;- runif(tdims, min=0, max=100) y &lt;- sqrt(x) testingX &lt;- array(0, dim=c(tdims,1)) #Store as arrays testingX[1:tdims,1] &lt;- x testingY &lt;- array(0, dim=c(tdims,1)) testingY[1:tdims,1] &lt;- y A user friendly package for neural networks is available via keras, an application programming interface (API) written in Python, which uses either theano or tensorflow as a back-end. An R interface for keras is available in the form of kerasR. Before we can use {kerasR} we first need to load the {keras} and {kerasR} libraries in R (prior to this we also have to install pyhton package {keras} and either {theano} or {tensorflow}). And so we come to specifying the model itself. Keras has an simple and intuitive way of specifying layers of a neural network, and kerasR makes good use of this. We first initialise the model: mod &lt;- Sequential() This tells keras that we’re using the Sequential API i.e., a network with the first layer connected to the second, the second to the third and so forth, which distinguishes it from more complex networks possible using the Model API. Once we’ve specified a sequential model, we can start adding layers to the neural network. A standard layer of neurons, can be specified using the {Dense} command: the first layer of our network must also include the dimension of the input data. So, for example, if our input data was a scalar, we could add an input layer via: mod$add(Dense(100, input_shape = c(1))) We also need to specify the activation function to the next level. This can be done via {Activation()}, so our snippet of code using a Rectified Linear Unit (relu) activation would look something like: mod$add(Dense(100, input_shape = c(1))) mod$add(Activation(&quot;relu&quot;)) This is all we need to specify a single layer of the neural network. We could add another layer of \\(120\\) neurons via: mod$add(Dense(120)) mod$add(Activation(&quot;relu&quot;)) Finally, we should add the output neurons. The number of output neurons will differ, but should match the size of the output we’re aiming to predict. In this section we have one output, a scalar representing the square root of the input, so will have a {Dense(1)} output. The final activation function also depends on the nature of our data. If, for example, we’re doing regression, we can explicitly specify a {linear} activation function. Our final model would look like: mod &lt;- Sequential() mod$add(Dense(100, input_shape = c(1))) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(120)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(1)) mod$add(Activation(&quot;linear&quot;)) That’s it. Simple! Next, we can print a summary of the network, to visualise how many parameters it has and other aspects: mod ## Model ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_1 (Dense) (None, 100) 200 ## ________________________________________________________________________________ ## activation_1 (Activation) (None, 100) 0 ## ________________________________________________________________________________ ## dense_2 (Dense) (None, 120) 12120 ## ________________________________________________________________________________ ## activation_2 (Activation) (None, 120) 0 ## ________________________________________________________________________________ ## dense_3 (Dense) (None, 1) 121 ## ________________________________________________________________________________ ## activation_3 (Activation) (None, 1) 0 ## ================================================================================ ## Total params: 12,441 ## Trainable params: 12,441 ## Non-trainable params: 0 ## ________________________________________________________________________________ Before we can perform inference, we need to compile and run the model. In this case we need to specify three things: A loss function, which specifies the objective function that the model will try to minimise. A number of existing loss functions are built into keras, including the mean squared error (mean_squared_error) for regression, and categorical cross entropy (categorical_crossentropy), which is used for categorical classification. Since we are dealing with regression, we will stick with the mean squared error. An optimiser, which determines how the loss function is optimised. Possible examples include stochastic gradient descent ({SGD()}) and Root Mean Square Propagation ({RMSprop()}). A list of metrics to return. These are additional summary statistics that keras evaluates and prints. For classification, a good choice would be accuracy (or {binary_accuracy}). We can compile our model using {keras_compile}: keras_compile(mod, loss = &#39;mean_squared_error&#39;, metrics = c(&#39;mean_squared_error&#39;), optimizer = RMSprop()) Finally the model can be fitted to the data. When doing so we additionally should specify the validation set (if we have one), the batch size, and the number of epochs, where an epoch is one forward pass and one backward pass of all the training examples, and the batch size is the number of training examples in one forward/backward pass. You may want to go and get a tea whilst this is running! set.seed(12345) keras_fit(mod, trainingX, trainingY, validation_data = list(testingX, testingY), batch_size = 100, epochs = 25, verbose = 1) We can see that the mean square error rapidly decreases (from approx. 39 at epoch 1 to around 1.7 at epoch 22). Let’s visualise the data versus the real values. newX &lt;- as.matrix(seq(from = 0, to = 200, by = 5)) predY &lt;- keras_predict(mod, x = newX) plot(newX,predY) lines(newX,sqrt(newX)) It’s not particularly good. However, we didn’t use a particularly large training set and there are a few things we can do to try to optimise the network. Another important point is that we didn’t use the best network (the one with the best test set error). By default when we call prediction functions we tend to use whatever the final network was during our training. We will later learn how to save the best network, so that we can load it in for prediction. For now we will continue on. Exercise 2.1: Try varying a few other things to get an idea of how NNs behave using keras. Try increasing the training set size, add or remove layers, and change their size. Another thing thing that can be varied is final layer activation. The [keras manual]{https://keras.io/api/layers/activations/} should provide a useful resource to explore what options are available. 4.1.2 Image classification with Rick and Morty We will now try to train a network for image classification. As with any machine learning application, it’s important to both have some question in mind (in this case “can we identify images that contain Rick Sanchez”), and understand the dataset(s) we’re using. The image data can be found in the directory {data/RickandMorty/data/}. We begin by loading in some images of Rick using the {readJPEG} and {grid.raster} functions. im &lt;- readJPEG(&quot;data/RickandMorty/data/AllRickImages/Rick_1.jpg&quot;) grid::grid.newpage() grid.raster(im, interpolate=FALSE, width = 0.5) Let’s understand take a closer look at this dataset. We can use the funciton {dim(im)} to return the image dimensions. In this case each image is stored as a jpeg file, with \\(90 \\times 160\\) pixel resolution and \\(3\\) colour channels (RGB). This loads into R as \\(160 \\times 90 \\times 3\\) array. We could start by converting the image to grey scale, reducing the dimensions of the input data. However, each channel will potentially carry novel information, so ideally we wish to retain all of the information. You can take a look at what information is present in the different channels by plotting them individually using e.g., {grid.raster(im[,,3], interpolate=FALSE)}. Whilst the difference is not so obvious here, we can imagine sitations where different channels could be dramamtically different, for example, when dealing with remote observation data from satellites, where we might have visible wavelength alongside infrared and a variety of other spectral channels. Since we plan to retain the channel information, our input data is a tensor of dimension \\(90 \\times 160 \\times 3\\) i.e., height x width x channels. Note that this ordering is important, as the the package we’re using expects this ordering (be careful, as other packages can expect a different ordering). Before building a neural network we first have to load the data and construct a training, validation, and test set of data. Whilst the package we’re using has the ability to specify this on the fly, I prefer to manually seperate out training/test/validation sets, as it makes it easier to later debug when things go wrong. First load all Rick images and all not Rick images from their directory. We can get a list of all the Rick and not Rick images using {list.files}: files1 &lt;- list.files(path = &quot;data/RickandMorty/data/AllRickImages/&quot;, pattern = &quot;jpg&quot;) files2 &lt;- list.files(path = &quot;data/RickandMorty/data/AllMortyImages/&quot;, pattern = &quot;jpg&quot;) After loading the lsit of files we can see we have \\(2211\\) images of Rick and \\(3046\\) images of not Rick. Whilst this is a slightly unbiased dataset it is not dramatically so; in cases where there is extreme inbalance in the number of class observations we may have to do something extra, such as data augmentation, or assinging weights during training. We next preallocate an empty array to store these training images for the Rick and not Rick images (an array of dimension \\(5257 \\times 90 \\times 160 \\times 3\\)): allX &lt;- array(0, dim=c(length(files1)+length(files2),dim(im)[1],dim(im)[2],dim(im)[3])) We can load images using the {readJPEG} function: for (i in 1:length(files1)){ allX[i,1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/AllRickImages/&quot;, files1[i], sep=&quot;&quot;)) } Similarly, we can load the not Rick images and store in the same array: for (i in 1:length(files2)){ allX[i+length(files1),1:dim(im)[1],1:dim(im)[2],1:dim(im)[3]] &lt;- readJPEG(paste(&quot;data/RickandMorty/data/AllMortyImages/&quot;, files2[i], sep=&quot;&quot;)) } Next we can construct a vector of length \\(5257\\) containing the classification for each of the images e.g., a \\(0\\) if the image is a Rick and \\(1\\) if it is not Rick. This is simple enough using the function {rbind}, as we know the first \\(2211\\) images were Rick and the second lot of images are not Rick. Since we are dealing with a classification algorithm, we next convert the data to binary categorical output (that is, a Rick is now represented as \\([1, 0]\\) and a not Rick is a \\([0, 1]\\)), which we can do using the {to_categorical} conversion function: #library(reticulate) #reticulate::use_python(&#39;/usr/bin/python3&#39;) #library(kerasR) labels &lt;- rbind(matrix(0, length(files1), 1), matrix(1, length(files2), 1)) allY &lt;- to_categorical(labels, num_classes = 2) Obviously in the snippet of code above we have \\(2\\) classes; we could just as easily perform classificaiton with more than \\(2\\) classes, for example if we wanted to classify Ricky, Morty, or Jerry, and so forth. We must now split our data in training sets, validation sets, and test sets. In fact I have already stored some seperate “test” set images in another folder that we will load in at the end, so here we only need to seperate images into training and validation sets. It’s important to note that we shouldn’t simply take the first \\(N\\) images for training with the remainder used for validation/testing, since this may introduce artefacts. For example, here we’ve loaded in all the Rick images in first, with the not Rick images loaded in second: if we took, say, the first \\(2000\\) images for training, we would be training with only Rick images, which makes our task impossible, and our algorithm will fail catastrophically. Although there are more elegant ways to shuffle data using {caret}, here we are going to manually randomly permute the data, and then take the first \\(4000\\) permuted images for training, with the remainder for validation (Note: it’s crucial to permute the \\(Y\\) data in the same way). set.seed(12345) #Set random number generator for the session vecInd &lt;- seq(0,length(files1)+length(files2)) #A vector of indexes trainInd &lt;- sample(vecInd)[1:4001] #Permute and take first 4000 training valInd &lt;- setdiff(vecInd,trainInd) #The remainder are for val/testing trainX &lt;- allX[trainInd, , , ] trainY &lt;- allY[trainInd, 1] valX &lt;- allX[valInd, , , ] valY &lt;- allY[valInd, 1] Before we move on, take a moment to think about the form of our data, in particular the output data Y. What exactly is the format we’ve settled on? This will be important later on in specifying our loss function. Think about cases where using similar datasets, we might want the data in a slightly different format. We are almost ready to begin building our neural networks. First can try a few things to make sure out data has been processed correctly. For example, try manually plotting several of the images and seeing if the labels are correct. Manually print out the image matrix (not a visualisation of it): think about the range of the data, and whether it will need normalising. Finally we can check to see how many of each class is in the training and validation datasets. In this case there are \\(1706\\) images of Rick and \\(2294\\) images of not Rick in the training dataset. Again, whilst there is some slight class inbalance it is not terrible, so we don’t need to perform data augmentation or assign weights to the different classes during training. 4.1.3 Rick and Morty classifier using Deep Learning Let us return to our example of image classification. We start by specifying a sequential network as before. mod &lt;- Sequential() Our data is slightly different to the usual inputs we’ve been dealing with: that is, we’re not dealing with an input vector, but instead have an array. In this case each image is a \\(90 \\times 160 \\time 3\\) array. So for our first layer we first have to flatten this down using {Flatten()}: mod$add(Flatten(input_shape = c(90, 160, 3))) This should turn our \\(90 \\times \\160 \\times 3\\) input into a \\(1 \\times 43200\\) node input. We now add an intermediate layer containing \\(100\\) neurons, connected to the input layer with rectified linear units ({relu}): mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(100)) Finally we connect this layer over the final output layer (two neurons) with sigmoid activation: activation mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(1)) mod$add(Activation(&quot;sigmoid&quot;)) The complete model should look something like: mod &lt;- Sequential() mod$add(Flatten(input_shape = c(90, 160, 3))) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(100)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(1)) mod$add(Activation(&quot;sigmoid&quot;)) We can visualise this model using the {plot_model} function (Figure 4.1). plot_model(mod,&#39;images/DNN1.png&#39;) Figure 4.1: Example of a multilayer convolutional neural network We can also print a summary of the network, for example to see how many parameters it has: mod ## Model ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## flatten_1 (Flatten) (None, 43200) 0 ## ________________________________________________________________________________ ## activation_4 (Activation) (None, 43200) 0 ## ________________________________________________________________________________ ## dense_4 (Dense) (None, 100) 4320100 ## ________________________________________________________________________________ ## activation_5 (Activation) (None, 100) 0 ## ________________________________________________________________________________ ## dense_5 (Dense) (None, 1) 101 ## ________________________________________________________________________________ ## activation_6 (Activation) (None, 1) 0 ## ================================================================================ ## Total params: 4,320,201 ## Trainable params: 4,320,201 ## Non-trainable params: 0 ## ________________________________________________________________________________ In this case we see a total of \\(4,320,201\\) parameters. Yikes, that’s a lot of parameters to tune, and not much data! Next we need to compile and run the model. In this case we need to specify the loss, optimiser, and metrics. Since we are dealing with binary classification, we will use binary cross entropy (binary_crossentropy) and for classification, a good choice of metrics would be accuracy (or {binary_accuracy}). We can compile our model using {keras_compile}: keras_compile(mod, loss = &#39;binary_crossentropy&#39;, metrics = c(&#39;binary_accuracy&#39;), optimizer = RMSprop()) Finally the model can be fitted to the data. When doing so we additionally need to specify the validation set (if we have one), the batch size and the number of epochs, where an epoch is one forward pass and one backward pass of all the training examples, and the batch size is the number of training examples in one forward/backward pass. You may want to go and get a tea whilst this is running! set.seed(12345) keras_fit(mod, trainX, trainY, validation_data = list(valX, valY), batch_size = 500, epochs = 25, verbose = 1) For this model we achieved an accuracy of above \\(0.57\\) on the validation dataset at epoch \\(25\\) (which had a corresponding accuracy \\(&gt;0.58\\) on the training set). Not great is an understatement. In fact, if we consider the slight imbalance in the number of classes, a niave algorithm that always assigns the data to not Rick would achieve an accuracy of \\(0.58\\) and \\(0.57\\) in the training and validation sets respectively. Another striking observation is that the accuracy itself doesn’t appear to be changing much during training: a possible sign that something is amiss. Let’s try adding in another layer to the network, just to see what happens. Before we do so, another important point to note is that the model we have at the end of training is the one one we generated for the latest epoch, which is not necessarily the model that gives us the best validation accuracy. Since our aim is to have the best predictive model we will also have to introduce a callback. In the snippet of code, below, we contruct a new network, with an additional layer containing \\(70\\) neurons, and introduce a callback that returns the best model at the end of our training: mod &lt;- Sequential() mod$add(Flatten(input_shape = c(90, 160, 3))) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(100)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(70)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(1)) mod$add(Activation(&quot;sigmoid&quot;)) callbacks &lt;- list(ModelCheckpoint(&#39;data/RickandMorty/data/models/model.h5&#39;, monitor = &quot;val_binary_accuracy&quot;, verbose = 0, save_best_only = TRUE, save_weights_only = FALSE, mode = &quot;auto&quot;, period = 1)) keras_compile(mod, loss = &#39;binary_crossentropy&#39;, metrics = c(&#39;binary_accuracy&#39;), optimizer = RMSprop()) set.seed(12345) keras_fit(mod, trainX, trainY, validation_data = list(valX, valY), batch_size = 500, epochs = 25, callbacks = callbacks, verbose = 1) We can again visualise the model: plot_model(mod,&#39;images/DNN2.png&#39;) Figure 4.2: Example of a multilayer convolutional neural network Note that we can load a saved model in using the {keras_load} function: mod = keras_load(&#39;data/RickandMorty/data/models/model.h5&#39;) However, the validation accuracy of this model is around \\(0.57\\). It seems like we’re getting nowhere fast, and need to change tactic. We need to think a little more about what the data actually is. In this case we’re looking at a set of images. As Rick Sanchez can appear almost anywhere in the image, there’s no reason to think that a given input node should correspond in two different images, so it’s not surprising that the network did so badly, this is simply a task that a densely connected network is poor at. We need something that can extract out features from the image irregardless of where Rick is. There are approaches build precisely for image analysis that do just this: convolutional neural networks. 4.2 Convolutional neural networks Convolutional neural networks essentially scan through an image and extract out a set of feature representations. In multilayer neural networks, these features might then be passed on to deeper layer (other convolutional layers or standard neurons) which extract out higher order features, as shown in Figure 4.3. Finally, a densly connected network acts to combine features together for prediction. At least in an idealised description of what’s going on. Figure 4.3: Example of a multilayer convolutional neural network In kerasR we can add a convolutional layer using {Conv2D}. A multilayer convolutional neural network might look something like: mod &lt;- Sequential() mod$add(Conv2D(filters = 20, kernel_size = c(5, 5),input_shape = c(90, 160, 3))) mod$add(Activation(&quot;relu&quot;)) mod$add(MaxPooling2D(pool_size=c(3, 3))) mod$add(Conv2D(filters = 20, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(MaxPooling2D(pool_size=c(3, 3))) mod$add(Conv2D(filters = 64, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(MaxPooling2D(pool_size=c(3, 3))) mod$add(Flatten()) mod$add(Dense(100)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dropout(0.3)) mod$add(Dense(1)) mod$add(Activation(&quot;sigmoid&quot;)) callbacks &lt;- list(ModelCheckpoint(&#39;data/RickandMorty/data/models/convmodel.h5&#39;, monitor = &quot;val_binary_accuracy&quot;, verbose = 0, save_best_only = TRUE, save_weights_only = FALSE, mode = &quot;auto&quot;, period = 1)) keras_compile(mod, loss = &#39;binary_crossentropy&#39;, metrics = c(&#39;binary_accuracy&#39;), optimizer = RMSprop()) set.seed(12345) keras_fit(mod, trainX, trainY, validation_data = list(valX, valY), batch_size = 100, epochs = 25, callbacks = callbacks, verbose = 1) In the above, we have also included an additional callback, which plots the model performence as we progress. Again we can visualise this network: plot_model(mod,&#39;images/DNN3.png&#39;) Figure 4.4: Example of a multilayer convolutional neural network Okay, so now we have achieved a better accuracy: we have an accuracy of \\(0.91\\) on the validation dataset at epoch \\(24\\), with a training accuracy of \\(0.98\\). Whilst this is still not great (compared to how well a human could do on a similar task), it’s accurate enough to begin making predictions and visualising the results. First load in the best model: mod = keras_load(&#39;data/RickandMorty/data/models/convmodel.h5&#39;) We can use this model to make predictions for images not present in either the training or validation datasets. We load in the new set of images, which can be found in the {predictions} subfolder: files &lt;- list.files(path = &quot;data/RickandMorty/data/predictions/&quot;,pattern = &quot;jpg&quot;) predictX &lt;- array(0,dim=c(length(files),90,160,3)) for (i in 1:length(files)){ x &lt;- readJPEG(paste(&quot;data/RickandMorty/data/predictions/&quot;, files[i],sep=&quot;&quot;)) predictX[i,1:90,1:160,1:3] &lt;- x[1:90,1:160,1:3] } A hard classification can be assigned using the {keras_predict_classes} function, whilst the actual probability of assignment to either class can be evaluated using {keras_predict_proba} (this can be useful for images that might be ambiguous). probY &lt;- keras_predict_proba(mod, predictX) predictY &lt;- keras_predict_classes(mod, predictX) We can plot an example: choice = 13 grid::grid.newpage() if (predictY[choice]==1) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;black&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;grey&quot;)) } choice = 1 grid::grid.newpage() if (predictY[choice]==1) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;black&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;black&quot;)) } choice = 6 grid::grid.newpage() if (predictY[choice]==1) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;black&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;black&quot;)) } grid::grid.newpage() choice = 16 if (predictY[choice]==1) { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Rick&#39;,x = 0.4, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;black&quot;)) } else { grid.raster(predictX[choice,1:90,1:160,1:3], interpolate=FALSE) grid.text(label=&#39;Not Rick: must be a Jerry&#39;,x = 0.2, y = 0.77,just = c(&quot;left&quot;, &quot;top&quot;), gp=gpar(fontsize=15, col=&quot;yellow&quot;)) } 4.2.1 Checking the models Although our model seems to be doing reasonably, it always helps to see where things are going wrong. Let’s take a look at a few of the false positives and a few of the false negatives. probvalY &lt;- keras_predict_proba(mod, valX) predictvalY &lt;- keras_predict_classes(mod, valX) TP &lt;- which(predictvalY==1 &amp; valY==1) FN &lt;- which(predictvalY==0 &amp; valY==1) TN &lt;- which(predictvalY==0 &amp; valY==0) FP &lt;- which(predictvalY==1 &amp; valY==0) Let’s see where we go it right: grid::grid.newpage() grid.raster(valX[TP[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[TP[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[TP[3],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) And wrong (false negative): grid::grid.newpage() grid.raster(valX[FN[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[FN[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[FN[3],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) Or false positives: grid::grid.newpage() grid.raster(valX[FP[1],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(valX[FP[2],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid.raster(valX[FP[4],1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.8) It’s not entirely clear why exactly the network is failing in some of these cases. An alternative way to look at what’s going wrong is a look at which pixels are contributing the most to the classifier, as we have done during the lecture. Currently this can be done in Python implementations of Keras using the [DeepExplain]{https://github.com/marcoancona/DeepExplain} package {M. Ancona and Grosss (2018)}. Example Python code for doing this has been provided in the {Python} subdirectory. 4.2.2 Data augmentation Although we saw some improvements when using convolutional neural networks compared to densely connected one, the end results were not particularly convincing. After all, previous applications in the recognition of handwritten digits (0-9) showed above human accuracy, see e.g., Neural Networks and Deep Learning. Our accuracy of approximately \\(90\\) percent is nowhere near human levels. So where are we gong wrong? We should, of course, start by considering the number of parameters versus the size of the training dataset. In our final model we had \\(69,506\\) parameters, and only a few thousand training images, so it is perhaps not surprising that our model is doing relatively poorly. In previous examples of digit recognition more than \\(10,000\\) images were used, whilst better known examples of deep learning for image classification make use of millions of images. Our task is also, arguably, a lot harder than digit recognition. After all, a handwritten \\(0\\) is relatively similar regardless of who wrote it. Rick Sanchez, on the other hand, can come in a diverse range of guises, with different postures, facial expressions, clothing, and even in pickle-Rick form. We may well need a vastly increased number of training images: with more training data, we can begin to learn more robustly what features define a Rick. Whilst we could simply download more data from Master of All Science, an alternative approach is to artificially increase our pool of training data by manipulating the images. For example, we could shear, warp or rotate some of the images in our training set; we could add noise and we could manipulate the colouring. 4.2.3 Asking more precise questions Another way we could improve our accuracy is to ask more precise questions. In our application we have focused on what makes a Rick, and what makes a not Rick. Whilst there may be definable features for Rick, such as his hair and his white coat, the class not Rick is an amalgamation of all other characters and scenes in the series. A more specific approach might be to develop algorithms that classify Rick versus Morty. In this case additionally learning the features of a Morty might make it easier to make a binary choice. Of course, we might want to allow more complex situations, such as case where you have a Rick and a Morty. As a general open question, think about how you would encode just such an example. What would you need to change in the code? Another approach that might help us increase our accuracy is to use transfer learning. This is where we make use of existing neural networks to make predictions about our specific datasets, usually by fixing the topology and parameters of the uppermost layers and fine tuning the lower layers to our dataset. For image recognition we could make use of top perfoming neural networks on the ImageNet database, although these types of large-scale models are certainly not without their issues {Prabhu and Birhane (2020)}. Whilst none of these networks would have been designed to identify Rick they would have been trained on millions of images, and the top levels would have been able to extract useful general features of that allowed identification of images. 4.2.4 More complex networks More complex learning algorithms can easily be built using Keras via the model class API. This allows, for example, learning from multiple inputs and/or predicting multiple outputs, with more interconnection between the different layers. We might, for example, want to include additional contextual information about the image that could serve to augment the predictions. ###Autoencoders In previous sections we have used CNNs to build a Rick/not Rick classifier. In doing so we are halfway towards other interesting neural network architectures, including autoencoders. One type of autoencoder consists of a stack of convolution/max pooling layers which served to condense the original image down into a reduced dimensional (encoded) representation, with a stack of upsampled layers used to decode the encoded layer (Figure 4.5). Within such a network the input and output layers are an identical image and we are therefore training a network that can both compresses the original high resolution data and subsequently interpret that compressed representation to recreate the original as closely as possible. A slight deviation of this principle would be to use noisy versions of the image as input, with clean versions as the output. In these cases the autoencoder becomes a denoiser (Figure 4.6). Similar methods can be used for generating higher resolution versions of an image. Figure 4.5: Example of an autoencoder (https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368) Figure 4.6: Example of an autoencoder (https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368) In the example below we implement a simple Autoencoder: mod &lt;- Sequential() mod$add(Conv2D(filters = 20, kernel_size = c(5, 5),input_shape = c(90, 160, 3))) mod$add(Activation(&quot;relu&quot;)) mod$add(Conv2D(filters = 20, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(Conv2D(filters = 64, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(Conv2DTranspose(filters = 64, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(Conv2DTranspose(filters = 20, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(Conv2DTranspose(filters = 20, kernel_size = c(5, 5))) mod$add(Activation(&quot;relu&quot;)) mod$add(Conv2D(filters = 3, kernel_size = c(5, 5), padding=&#39;same&#39;)) mod$add(Activation(&quot;sigmoid&quot;)) callbacks &lt;- list(ModelCheckpoint(&#39;data/RickandMorty/data/models/AEmodel.h5&#39;, monitor = &quot;val_binary_crossentropy&quot;, verbose = 0, save_best_only = TRUE, save_weights_only = FALSE, mode = &quot;auto&quot;, period = 1)) keras_compile(mod, loss = &#39;binary_crossentropy&#39;, metrics = c(&#39;binary_accuracy&#39;), optimizer = RMSprop()) set.seed(12345) keras_fit(mod, trainX, trainX, validation_data = list(valX, valX), batch_size = 500, epochs = 10, callbacks = callbacks, verbose = 1) For simplicity we have discarded the MaxPool layers. This condenses the images from \\(90 \\times 160\\) pixel images down to \\(78 \\times 148\\) (not a huge compression but enough to make our point). Let’s plot a few of the examples to see how well we have done at condensing then reconstructing images on some of the test data: predictAEX &lt;- keras_predict(mod, predictX) grid::grid.newpage() grid.raster(predictX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(predictAEX[1,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid::grid.newpage() grid.raster(predictX[2,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(predictAEX[2,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) grid::grid.newpage() grid.raster(predictX[3,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.2) grid.raster(predictAEX[3,1:90,1:160,1:3], interpolate=FALSE, width = 0.3, x = 0.5, y=0.5) In the example below we code up a simple autoencoder (note that run time is likely too long for use within this practical, hence evaluation has been set to false). Exercise 2.2: Think about how the script can be modified to demonstrate the use of a denoisiny algorithm (hint: the dataset will need to be modified in some way). 4.3 Further reading A particularly comprehensive introduction to Deep Learning can be found in the e-book Neural Networks and Deep Learning, written by Michael Nielsen. Useful examples can also be found in the keras documentation. Installing Python Linux Installing Python for Mac Python install via Conda Installing Tensorflow Installing Keras ======= ## Exercises Solutions to exercises can be found in appendix 5. References "],
["solutions-logistic-regression.html", "5 Solutions to Chapter 4 - Linear regression and logistic regression", " 5 Solutions to Chapter 4 - Linear regression and logistic regression Solutions to exercises of chapter 3. Exercise 1.1. Before we begin, we first need to visualise the data as a whole. Heatmaps are one way of looking at large datasets. Since we’re looking for differences I will make a heatmap of the difference between control and infected at each time point and subcluster by pattern: library(pheatmap) DeltaVals &lt;- t(D[25:48,3:164] - D[1:24,3:164]) pheatmap(DeltaVals, cluster_cols = FALSE, cluster_rows = TRUE) we can see a number of rows in which there appears to be large scale changes as the time series progresses. Pick one where this is particularly strong. Exercise 1.1. We can systematically fit a model with increasing degree and evaluate/plot the RMSE on the held out data. library(pheatmap) RMSE &lt;- rep(NULL, 10) lrfit1 &lt;- train(y~poly(x,degree=1), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[1] &lt;- lrfit1$results$RMSE lrfit2 &lt;- train(y~poly(x,degree=2), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[2] &lt;- lrfit2$results$RMSE lrfit3 &lt;- train(y~poly(x,degree=3), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[3] &lt;- lrfit3$results$RMSE lrfit4 &lt;- train(y~poly(x,degree=4), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[4] &lt;- lrfit4$results$RMSE lrfit5 &lt;- train(y~poly(x,degree=5), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[5] &lt;- lrfit5$results$RMSE lrfit6 &lt;- train(y~poly(x,degree=6), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[6] &lt;- lrfit6$results$RMSE lrfit7 &lt;- train(y~poly(x,degree=7), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[7] &lt;- lrfit7$results$RMSE lrfit8 &lt;- train(y~poly(x,degree=8), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[8] &lt;- lrfit8$results$RMSE lrfit9 &lt;- train(y~poly(x,degree=9), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[9] &lt;- lrfit9$results$RMSE lrfit10 &lt;- train(y~poly(x,degree=10), data=data.frame(x=D[1:24,1],y=D[1:24,geneindex]), method = &quot;lm&quot;) RMSE[10] &lt;- lrfit10$results$RMSE plot(RMSE) plot(RMSE[1:5]) #barplot(c(lrfit2$results$RMSE,lrfit3$results$RMSE,lrfit4$results$RMSE)) From these plots it looks like the best model is one with degree \\(d=2\\) or \\(d=4\\), suggesting there is a lot more complexity to this gene. "],
["solutions-to-chapter-5-neural-networks.html", "6 Solutions to Chapter 5 - Neural Networks", " 6 Solutions to Chapter 5 - Neural Networks Excersie 2.1: We increase the training size and tweak network structure in various ways. tdims &lt;- 5000 #Number of samples to generate x &lt;- runif(tdims, min=0, max=100) #Generate random x in range 0 to 100 y &lt;- sqrt(x) #Calculate square root of x trainingX &lt;- array(0, dim=c(tdims,1)) #Store data as an array (required by Keras) trainingX[1:tdims,1] &lt;- x trainingY &lt;- array(0, dim=c(tdims,1)) trainingY[1:tdims,1] &lt;- y #Now do the same but for a independently generated test set x &lt;- runif(tdims, min=0, max=100) y &lt;- sqrt(x) testingX &lt;- array(0, dim=c(tdims,1)) #Store as arrays testingX[1:tdims,1] &lt;- x testingY &lt;- array(0, dim=c(tdims,1)) testingY[1:tdims,1] &lt;- y mod &lt;- Sequential() mod$add(Dense(10, input_shape = c(1))) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(20)) mod$add(Activation(&quot;relu&quot;)) mod$add(Dense(1)) mod$add(Activation(&quot;linear&quot;)) keras_compile(mod, loss = &#39;mean_squared_error&#39;, metrics = c(&#39;mean_squared_error&#39;), optimizer = RMSprop()) set.seed(12345) keras_fit(mod, trainingX, trainingY, validation_data = list(testingX, testingY), batch_size = 1000, epochs = 450, verbose = 1) newX &lt;- as.matrix(seq(from = 0, to = 200, by = 5)) predY &lt;- keras_predict(mod, x = newX) plot(newX,predY) lines(newX,sqrt(newX)) For comparison we can also use linear regression to compare our predictions: colnames(trainingX) &lt;- &quot;x&quot; colnames(trainingY) &lt;- &quot;y&quot; lrfit &lt;- lm(y~x) newd &lt;- data.frame(x=newX) predictedValues&lt;-predict.lm(lrfit, newdata = newd) #RMSE = sqrt( mean((testingY - predictedValues)^2) ) lines(newX,predictedValues, col=&quot;red&quot;) Excercsie 2.1: The network architecture should be fine for this task. However a noisy version of the input data will have to be generated (e.g., by setting a random set of pixels to zero) to be passed in to the AE. A clean version of the data should be retained and passed to the AE as the output. "]
]
